---
title: ML.NET 指标
description: 了解用于评估 ML.NET 模型性能的指标
ms.date: 12/17/2019
ms.openlocfilehash: 8e823fd8cc344c1b8e0ecd709b527137368cbfa0
ms.sourcegitcommit: 9a97c76e141333394676bc5d264c6624b6f45bcf
ms.translationtype: HT
ms.contentlocale: zh-CN
ms.lasthandoff: 01/08/2020
ms.locfileid: "75739607"
---
# <a name="evaluate-your-mlnet-model-with-metrics"></a><span data-ttu-id="50fd1-103">使用指标评估 ML.NET 模型</span><span class="sxs-lookup"><span data-stu-id="50fd1-103">Evaluate your ML.NET model with metrics</span></span>

<span data-ttu-id="50fd1-104">了解用于评估 ML.NET 模型的指标。</span><span class="sxs-lookup"><span data-stu-id="50fd1-104">Understand the metrics used to evaluate an ML.NET model.</span></span>

<span data-ttu-id="50fd1-105">评估指标特定于模型所执行的机器学习任务的类型。</span><span class="sxs-lookup"><span data-stu-id="50fd1-105">Evaluation metrics are specific to the type of machine learning task that a model performs.</span></span>

<span data-ttu-id="50fd1-106">例如，对于分类任务，通过衡量预测类别与实际类别的匹配程度来评估模型。</span><span class="sxs-lookup"><span data-stu-id="50fd1-106">For example, for the classification task, the model is evaluated by measuring how well a predicted category matches the actual category.</span></span> <span data-ttu-id="50fd1-107">对于聚类分析，评估基于群集项的接近程度，以及群集之间的距离。</span><span class="sxs-lookup"><span data-stu-id="50fd1-107">And for clustering, evaluation is based on how close clustered items are to each other, and how much separation there is between the clusters.</span></span>

## <a name="evaluation-metrics-for-binary-classification"></a><span data-ttu-id="50fd1-108">二元分类评估指标</span><span class="sxs-lookup"><span data-stu-id="50fd1-108">Evaluation metrics for Binary Classification</span></span>

| <span data-ttu-id="50fd1-109">指标</span><span class="sxs-lookup"><span data-stu-id="50fd1-109">Metrics</span></span>   |      <span data-ttu-id="50fd1-110">描述</span><span class="sxs-lookup"><span data-stu-id="50fd1-110">Description</span></span>      |  <span data-ttu-id="50fd1-111">期望</span><span class="sxs-lookup"><span data-stu-id="50fd1-111">Look for</span></span> |
|-----------|-----------------------|-----------|
| <span data-ttu-id="50fd1-112">**准确性**</span><span class="sxs-lookup"><span data-stu-id="50fd1-112">**Accuracy**</span></span> |  <span data-ttu-id="50fd1-113">[准确性](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification)指正确预测与测试数据集的比例。</span><span class="sxs-lookup"><span data-stu-id="50fd1-113">[Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification) is the proportion of correct predictions with a test data set.</span></span> <span data-ttu-id="50fd1-114">它是正确预测次数与输入示例总数的比率。</span><span class="sxs-lookup"><span data-stu-id="50fd1-114">It is the ratio of number of correct predictions to the total number of input samples.</span></span> <span data-ttu-id="50fd1-115">如果属于每个类的示例数量相似，此方法会很有效。</span><span class="sxs-lookup"><span data-stu-id="50fd1-115">It works well if there are similar number of samples belonging to each class.</span></span>| <span data-ttu-id="50fd1-116">**越接近 1.00 越好**。</span><span class="sxs-lookup"><span data-stu-id="50fd1-116">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="50fd1-117">但刚好 1.00 表示存在问题（通常包括：标签/目标泄漏、过度拟合或使用训练数据进行测试）。</span><span class="sxs-lookup"><span data-stu-id="50fd1-117">But exactly 1.00 indicates an issue (commonly: label/target leakage, over-fitting, or testing with training data).</span></span> <span data-ttu-id="50fd1-118">当测试数据处于非平衡状态（大部分实例属于其中一个类）、数据集较小，或分数趋近 0.00 或 1.00 时，准确性并不能真实反应分类器的效果，此时需要检查其他指标。</span><span class="sxs-lookup"><span data-stu-id="50fd1-118">When the test data is unbalanced (where most of the instances belong to one of the classes), the dataset is small, or scores approach 0.00 or 1.00, then accuracy doesn’t really capture the effectiveness of a classifier and you need to check additional metrics.</span></span> |
| <span data-ttu-id="50fd1-119">**AUC**</span><span class="sxs-lookup"><span data-stu-id="50fd1-119">**AUC**</span></span> |    <span data-ttu-id="50fd1-120">[aucROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) 或曲线下面积  通过扫描真正率和假正率来测量曲线下面积。</span><span class="sxs-lookup"><span data-stu-id="50fd1-120">[aucROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) or *Area under the curve* measures the area under the curve created by sweeping the true positive rate vs. the false positive rate.</span></span>  |   <span data-ttu-id="50fd1-121">**越接近 1.00 越好**。</span><span class="sxs-lookup"><span data-stu-id="50fd1-121">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="50fd1-122">它应大于 0.50 才能接受模型。</span><span class="sxs-lookup"><span data-stu-id="50fd1-122">It should be greater than 0.50 for a model to be acceptable.</span></span> <span data-ttu-id="50fd1-123">AUC 为 0.50 或更低的模型毫无意义。</span><span class="sxs-lookup"><span data-stu-id="50fd1-123">A model with AUC of 0.50 or less is worthless.</span></span> |
| <span data-ttu-id="50fd1-124">**AUCPR**</span><span class="sxs-lookup"><span data-stu-id="50fd1-124">**AUCPR**</span></span> | <span data-ttu-id="50fd1-125">[aucPR](https://www.coursera.org/lecture/ml-classification/precision-recall-curve-rENu8) 或*查准率-查全率曲线的曲线下面积*：在类不均衡的情况下（高度偏斜的数据集），是成功预测的有用度量值。</span><span class="sxs-lookup"><span data-stu-id="50fd1-125">[aucPR](https://www.coursera.org/lecture/ml-classification/precision-recall-curve-rENu8) or *Area under the curve of a Precision-Recall curve*: Useful measure of success of prediction when the classes are imbalanced (highly skewed datasets).</span></span> |  <span data-ttu-id="50fd1-126">**越接近 1.00 越好**。</span><span class="sxs-lookup"><span data-stu-id="50fd1-126">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="50fd1-127">接近 1.00 的高分表明分类器返回准确的结果（高查准率），以及返回大部分正结果（高查全率）。</span><span class="sxs-lookup"><span data-stu-id="50fd1-127">High scores close to 1.00 show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).</span></span> |
| <span data-ttu-id="50fd1-128">**F1 分数**</span><span class="sxs-lookup"><span data-stu-id="50fd1-128">**F1-score**</span></span> | <span data-ttu-id="50fd1-129">[F1 分数](https://en.wikipedia.org/wiki/F1_score)也称为*均衡 F 分数或 F 度量值*。</span><span class="sxs-lookup"><span data-stu-id="50fd1-129">[F1 score](https://en.wikipedia.org/wiki/F1_score) also known as *balanced F-score or F-measure*.</span></span> <span data-ttu-id="50fd1-130">这是查准率和查全率的调和平均数。</span><span class="sxs-lookup"><span data-stu-id="50fd1-130">It's the harmonic mean of the precision and recall.</span></span> <span data-ttu-id="50fd1-131">如果想要在查准率和查全率之间寻求平衡，F1 分数很有用。</span><span class="sxs-lookup"><span data-stu-id="50fd1-131">F1 Score is helpful when you want to seek a balance between Precision and Recall.</span></span>| <span data-ttu-id="50fd1-132">**越接近 1.00 越好**。</span><span class="sxs-lookup"><span data-stu-id="50fd1-132">**The closer to 1.00, the better**.</span></span>  <span data-ttu-id="50fd1-133">F1 分数的最佳值为 1.00，最差值为 0.00。</span><span class="sxs-lookup"><span data-stu-id="50fd1-133">An F1 score reaches its best value at 1.00 and worst score at 0.00.</span></span> <span data-ttu-id="50fd1-134">它可指示分类器的查准率。</span><span class="sxs-lookup"><span data-stu-id="50fd1-134">It tells you how precise your classifier is.</span></span> |

<span data-ttu-id="50fd1-135">有关二元分类指标的更多详细信息，请阅读以下文章：</span><span class="sxs-lookup"><span data-stu-id="50fd1-135">For further details on binary classification metrics read the following articles:</span></span>

- [<span data-ttu-id="50fd1-136">准确性、查准率、查全率还是 F1？</span><span class="sxs-lookup"><span data-stu-id="50fd1-136">Accuracy, Precision, Recall, or F1?</span></span>](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9)
- [<span data-ttu-id="50fd1-137">二元分类指标类</span><span class="sxs-lookup"><span data-stu-id="50fd1-137">Binary Classification Metrics class</span></span>](xref:Microsoft.ML.Data.BinaryClassificationMetrics)
- <span data-ttu-id="50fd1-138">[The Relationship Between Precision-Recall and ROC Curves](http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf)（查准率-查全率曲线与 ROC 曲线之间的关系）</span><span class="sxs-lookup"><span data-stu-id="50fd1-138">[The Relationship Between Precision-Recall and ROC Curves](http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf)</span></span>

## <a name="evaluation-metrics-for-multi-class-classification"></a><span data-ttu-id="50fd1-139">多类分类评估指标</span><span class="sxs-lookup"><span data-stu-id="50fd1-139">Evaluation metrics for Multi-class Classification</span></span>

| <span data-ttu-id="50fd1-140">指标</span><span class="sxs-lookup"><span data-stu-id="50fd1-140">Metrics</span></span>   |      <span data-ttu-id="50fd1-141">描述</span><span class="sxs-lookup"><span data-stu-id="50fd1-141">Description</span></span>      |  <span data-ttu-id="50fd1-142">期望</span><span class="sxs-lookup"><span data-stu-id="50fd1-142">Look for</span></span> |
|-----------|-----------------------|-----------|
| <span data-ttu-id="50fd1-143">**微观准确性**</span><span class="sxs-lookup"><span data-stu-id="50fd1-143">**Micro-Accuracy**</span></span> |  <span data-ttu-id="50fd1-144">[微平均准确性](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.MicroAccuracy)聚合所有类的贡献度来计算平均指标。</span><span class="sxs-lookup"><span data-stu-id="50fd1-144">[Micro-average Accuracy](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.MicroAccuracy) aggregates the contributions of all classes to compute the average metric.</span></span> <span data-ttu-id="50fd1-145">它是正确预测的实例的部分。</span><span class="sxs-lookup"><span data-stu-id="50fd1-145">It is the fraction of instances predicted correctly.</span></span> <span data-ttu-id="50fd1-146">微平均不考虑类成员资格。</span><span class="sxs-lookup"><span data-stu-id="50fd1-146">The micro-average does not take class membership into account.</span></span> <span data-ttu-id="50fd1-147">基本而言，每个“示例-类”对对准确性指标的贡献度相同。</span><span class="sxs-lookup"><span data-stu-id="50fd1-147">Basically, every sample-class pair contributes equally to the accuracy metric.</span></span> | <span data-ttu-id="50fd1-148">**越接近 1.00 越好**。</span><span class="sxs-lookup"><span data-stu-id="50fd1-148">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="50fd1-149">在多类分类任务中，如果怀疑可能存在类不均衡的现象（即</span><span class="sxs-lookup"><span data-stu-id="50fd1-149">In a multi-class classification task, micro-accuracy is preferable over macro-accuracy if you suspect there might be class imbalance (i.e</span></span> <span data-ttu-id="50fd1-150">某个类的实例可能比其他类的实例多很多），则应选择微观准确性，而不是宏观准确性。</span><span class="sxs-lookup"><span data-stu-id="50fd1-150">you may have many more examples of one class than of other classes).</span></span>|
| <span data-ttu-id="50fd1-151">**宏观准确性**</span><span class="sxs-lookup"><span data-stu-id="50fd1-151">**Macro-Accuracy**</span></span> | <span data-ttu-id="50fd1-152">[宏平均准确性](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.MacroAccuracy)指类级别的平均准确性。</span><span class="sxs-lookup"><span data-stu-id="50fd1-152">[Macro-average Accuracy](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.MacroAccuracy) is the average accuracy at the class level.</span></span> <span data-ttu-id="50fd1-153">每个类的准确性都会进行计算，宏观准确性就是这些准确性的平均值。</span><span class="sxs-lookup"><span data-stu-id="50fd1-153">The accuracy for each class is computed and the macro-accuracy is the average of these accuracies.</span></span> <span data-ttu-id="50fd1-154">基本而言，每个类对准确性指标的贡献度相同。</span><span class="sxs-lookup"><span data-stu-id="50fd1-154">Basically, every class contributes equally to the accuracy metric.</span></span> <span data-ttu-id="50fd1-155">占比较小的类与占比较大的类拥有同等的权重。</span><span class="sxs-lookup"><span data-stu-id="50fd1-155">Minority classes are given equal weight as the larger classes.</span></span> <span data-ttu-id="50fd1-156">无论数据集包含多少个来自该类的实例，宏平均指标都会对每个类赋予相同的权重。</span><span class="sxs-lookup"><span data-stu-id="50fd1-156">The macro-average metric gives the same weight to each class, no matter how many instances from that class the dataset contains.</span></span> |  <span data-ttu-id="50fd1-157">**越接近 1.00 越好**。</span><span class="sxs-lookup"><span data-stu-id="50fd1-157">**The closer to 1.00, the better**.</span></span>  <span data-ttu-id="50fd1-158">它为每个类单独计算该指标，然后取平均值（因此实现平等对待所有类）</span><span class="sxs-lookup"><span data-stu-id="50fd1-158">It computes the metric independently for each class and then takes the average (hence treating all classes equally)</span></span> |
| <span data-ttu-id="50fd1-159">**对数损失**</span><span class="sxs-lookup"><span data-stu-id="50fd1-159">**Log-loss**</span></span>| <span data-ttu-id="50fd1-160">[对数损失](http://wiki.fast.ai/index.php/Log_Loss)测量分类模型的性能，其中预测输入是介于 0.00 和 1.00 之间的概率值。</span><span class="sxs-lookup"><span data-stu-id="50fd1-160">[Logarithmic loss](http://wiki.fast.ai/index.php/Log_Loss) measures the performance of a classification model where the prediction input is a probability value between 0.00 and 1.00.</span></span> <span data-ttu-id="50fd1-161">随着预测概率偏离实际标签，对数损失会增加。</span><span class="sxs-lookup"><span data-stu-id="50fd1-161">Log-loss increases as the predicted probability diverges from the actual label.</span></span> | <span data-ttu-id="50fd1-162">**越接近 0.00 越好**。</span><span class="sxs-lookup"><span data-stu-id="50fd1-162">**The closer to 0.00, the better**.</span></span> <span data-ttu-id="50fd1-163">完美模型的对数损失为 0.00。</span><span class="sxs-lookup"><span data-stu-id="50fd1-163">A perfect model would have a log-loss of 0.00.</span></span> <span data-ttu-id="50fd1-164">我们的机器学习模型旨在最小化此值。</span><span class="sxs-lookup"><span data-stu-id="50fd1-164">The goal of our machine learning models is to minimize this value.</span></span>|
| <span data-ttu-id="50fd1-165">**对数损失减小**</span><span class="sxs-lookup"><span data-stu-id="50fd1-165">**Log-Loss Reduction**</span></span> | <span data-ttu-id="50fd1-166">[对数损失减小](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.LogLossReduction)可以解释为分类器相较随机预测的优势。</span><span class="sxs-lookup"><span data-stu-id="50fd1-166">[Logarithmic loss reduction](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.LogLossReduction) can be interpreted as the advantage of the classifier over a random prediction.</span></span>| <span data-ttu-id="50fd1-167">**取值范围为 [-inf, 1.00]，其中 1.00 表示完美的预测，0.00 表示准确性一般的预测**。</span><span class="sxs-lookup"><span data-stu-id="50fd1-167">**Ranges from -inf and 1.00, where 1.00 is perfect predictions and 0.00 indicates mean predictions**.</span></span> <span data-ttu-id="50fd1-168">例如，如果该值等于 0.20，则可以将其解释为“正确预测的概率比随机猜测的概率高 20%”</span><span class="sxs-lookup"><span data-stu-id="50fd1-168">For example, if the value equals 0.20, it can be interpreted as "the probability of a correct prediction is 20% better than random guessing"</span></span>|

<span data-ttu-id="50fd1-169">微观准确性通常能够更好地与 ML 预测的业务需求保持一致。</span><span class="sxs-lookup"><span data-stu-id="50fd1-169">Micro-accuracy is generally better aligned with the business needs of ML predictions.</span></span> <span data-ttu-id="50fd1-170">如果想要选择单个指标用于选择多类分类任务的质量，则通常应选择微观准确性。</span><span class="sxs-lookup"><span data-stu-id="50fd1-170">If you want to select a single metric for choosing the quality of a multiclass classification task, it should usually be micro-accuracy.</span></span>

<span data-ttu-id="50fd1-171">例如，对于支持票证分类任务：（将传入的票证映射到支持团队）</span><span class="sxs-lookup"><span data-stu-id="50fd1-171">Example, for a support ticket classification task: (maps incoming tickets to support teams)</span></span>

- <span data-ttu-id="50fd1-172">微观准确性 - 传入票证分类给正确团队的频率如何？</span><span class="sxs-lookup"><span data-stu-id="50fd1-172">Micro-accuracy -- how often does an incoming ticket get classified to the right team?</span></span>
- <span data-ttu-id="50fd1-173">宏观准确性 - 对于普通团队而言，传入票证符合其业务范围的频率如何？</span><span class="sxs-lookup"><span data-stu-id="50fd1-173">Macro-accuracy -- for an average team, how often is an incoming ticket correct for their team?</span></span>

<span data-ttu-id="50fd1-174">在此示例中，宏观准确性对于小型团队而言任务过重；一个每年仅收到 10 个票证的小团队被视为每年可收到 10,000 个票证的大团队。</span><span class="sxs-lookup"><span data-stu-id="50fd1-174">Macro-accuracy overweights small teams in this example; a small team that gets only 10 tickets per year counts as much as a large team with 10k tickets per year.</span></span> <span data-ttu-id="50fd1-175">在本例中，微观准确性与“公司可通过自动化票证路由流程节省多少时间/金钱”这一业务需求相关性更高。</span><span class="sxs-lookup"><span data-stu-id="50fd1-175">Micro-accuracy in this case correlates better with the business need of, "how much time/money can the company save by automating my ticket routing process".</span></span>

<span data-ttu-id="50fd1-176">有关多类分类指标的更多详细信息，请阅读以下文章：</span><span class="sxs-lookup"><span data-stu-id="50fd1-176">For further details on multi-class classification metrics read the following articles:</span></span>

- [<span data-ttu-id="50fd1-177">查准率、查全率和 F 分数的微平均及宏平均</span><span class="sxs-lookup"><span data-stu-id="50fd1-177">Micro- and Macro-average of Precision, Recall, and F-Score</span></span>](https://rushdishams.blogspot.com/2011/08/micro-and-macro-average-of-precision.html)
- <span data-ttu-id="50fd1-178">[Multiclass Classification with Imbalanced Dataset](https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a)（不均衡数据集的多类分类）</span><span class="sxs-lookup"><span data-stu-id="50fd1-178">[Multiclass Classification with Imbalanced Dataset](https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a)</span></span>

## <a name="evaluation-metrics-for-regression-and-recommendation"></a><span data-ttu-id="50fd1-179">回归和建议评估指标</span><span class="sxs-lookup"><span data-stu-id="50fd1-179">Evaluation metrics for Regression and Recommendation</span></span>

<span data-ttu-id="50fd1-180">回归和建议任务均预测数字。</span><span class="sxs-lookup"><span data-stu-id="50fd1-180">Both the regression and recommendation tasks predict a number.</span></span> <span data-ttu-id="50fd1-181">对于回归，数字可以是受输入属性影响的任何输出属性。</span><span class="sxs-lookup"><span data-stu-id="50fd1-181">In the case of regression, the number can be any output property that is influenced by the input properties.</span></span> <span data-ttu-id="50fd1-182">对于建议，数字通常是一个分级值（例如 1 和 5 之间），或者为是/否建议（分别用 1 和 0 表示）。</span><span class="sxs-lookup"><span data-stu-id="50fd1-182">For recommendation, the number is usually a rating value (between 1 and 5 for example), or a yes/no recommendation (represented by 1 and 0 respectively).</span></span>

| <span data-ttu-id="50fd1-183">指标</span><span class="sxs-lookup"><span data-stu-id="50fd1-183">Metric</span></span>   |      <span data-ttu-id="50fd1-184">描述</span><span class="sxs-lookup"><span data-stu-id="50fd1-184">Description</span></span>      |  <span data-ttu-id="50fd1-185">期望</span><span class="sxs-lookup"><span data-stu-id="50fd1-185">Look for</span></span> |
|----------|-----------------------|-----------|
| <span data-ttu-id="50fd1-186">**R 平方**</span><span class="sxs-lookup"><span data-stu-id="50fd1-186">**R-Squared**</span></span> |  <span data-ttu-id="50fd1-187">[R 平方 (R2)](https://en.wikipedia.org/wiki/Coefficient_of_determination) 又称为*决定系数*，其将模型的预测能力表示为取值范围介于 -inf 和 1.00 之间的值。</span><span class="sxs-lookup"><span data-stu-id="50fd1-187">[R-squared (R2)](https://en.wikipedia.org/wiki/Coefficient_of_determination), or *Coefficient of determination* represents the predictive power of the model as a value between -inf and 1.00.</span></span> <span data-ttu-id="50fd1-188">1.00 意味着完美拟合，且拟合度可以无穷差，因此分数可能为负数。</span><span class="sxs-lookup"><span data-stu-id="50fd1-188">1.00 means there is a perfect fit, and the fit can be arbitrarily poor so the scores can be negative.</span></span> <span data-ttu-id="50fd1-189">分数为 0.00 表示模型正在猜测标签的预期值。</span><span class="sxs-lookup"><span data-stu-id="50fd1-189">A score of 0.00 means the model is guessing the expected value for the label.</span></span> <span data-ttu-id="50fd1-190">R2 测量实际测试数据值与预测值的接近程度。</span><span class="sxs-lookup"><span data-stu-id="50fd1-190">R2 measures how close the actual test data values are to the predicted values.</span></span> | <span data-ttu-id="50fd1-191">**越接近 1.00 表示质量越好**。</span><span class="sxs-lookup"><span data-stu-id="50fd1-191">**The closer to 1.00, the better quality**.</span></span> <span data-ttu-id="50fd1-192">但是，有时较低的 R 平方值（例如 0.50）对于方案而言可能完全正常或足够好，并且较高的 R 平方值并不总是好结果且有时可疑。</span><span class="sxs-lookup"><span data-stu-id="50fd1-192">However, sometimes low R-squared values (such as 0.50) can be entirely normal or good enough for your scenario and high R-squared values are not always good and be suspicious.</span></span> |
| <span data-ttu-id="50fd1-193">**绝对值损失**</span><span class="sxs-lookup"><span data-stu-id="50fd1-193">**Absolute-loss**</span></span> |  <span data-ttu-id="50fd1-194">[绝对值损失](https://en.wikipedia.org/wiki/Mean_absolute_error)又称为*平均绝对误差 (MAE)* ，其测量预测结果与实际结果的接近程度。</span><span class="sxs-lookup"><span data-stu-id="50fd1-194">[Absolute-loss](https://en.wikipedia.org/wiki/Mean_absolute_error) or *Mean absolute error (MAE)* measures how close the predictions are to the actual outcomes.</span></span> <span data-ttu-id="50fd1-195">它是所有模型误差的平均值，其中模型误差是预测标签值和正确标签值之间的绝对差距。</span><span class="sxs-lookup"><span data-stu-id="50fd1-195">It is the average of all the model errors, where model error is the absolute distance between the predicted label value and the correct label value.</span></span> <span data-ttu-id="50fd1-196">为测试数据集的每个记录计算此预测误差。</span><span class="sxs-lookup"><span data-stu-id="50fd1-196">This prediction error is calculated for each record of the test data set.</span></span> <span data-ttu-id="50fd1-197">最后，计算所有已记录的绝对误差的平均值。</span><span class="sxs-lookup"><span data-stu-id="50fd1-197">Finally, the mean value is calculated for all recorded absolute errors.</span></span>| <span data-ttu-id="50fd1-198">**越接近 0.00 表示质量越好。**</span><span class="sxs-lookup"><span data-stu-id="50fd1-198">**The closer to 0.00, the better quality.**</span></span> <span data-ttu-id="50fd1-199">平均绝对误差使用与待测量数据相同的比例（未规范化为特定范围）。</span><span class="sxs-lookup"><span data-stu-id="50fd1-199">The mean absolute error uses the same scale as the data being measured (is not normalized to specific range).</span></span> <span data-ttu-id="50fd1-200">绝对值损失、平方损失和 RMS 损失只能用于同一数据集的模型或具有类似标签值分布的数据集之间的比较。</span><span class="sxs-lookup"><span data-stu-id="50fd1-200">Absolute-loss, Squared-loss, and RMS-loss can only be used to make comparisons between models for the same dataset or dataset with a similar label value distribution.</span></span> |
| <span data-ttu-id="50fd1-201">**平方损失**</span><span class="sxs-lookup"><span data-stu-id="50fd1-201">**Squared-loss**</span></span> |  <span data-ttu-id="50fd1-202">[平方损失](https://en.wikipedia.org/wiki/Mean_squared_error)或均方误差 (MSE)  （也称为“均方差 (MSD)”）  通过计算从点到回归线的距离（这些距离为误差 E）并对它们进行求平方，可以告诉你回归线与一组测试数据值的距离。</span><span class="sxs-lookup"><span data-stu-id="50fd1-202">[Squared-loss](https://en.wikipedia.org/wiki/Mean_squared_error) or *Mean Squared Error (MSE)*, also called *Mean Squared Deviation (MSD)*, tells you how close a regression line is to a set of test data values by taking the distances from the points to the regression line (these distances are the errors E) and squaring them.</span></span> <span data-ttu-id="50fd1-203">求平方可赋予较大的差异更大的权重。</span><span class="sxs-lookup"><span data-stu-id="50fd1-203">The squaring gives more weight to larger differences.</span></span> | <span data-ttu-id="50fd1-204">它始终是非负值，并且**越接近 0.00 的值越好**。</span><span class="sxs-lookup"><span data-stu-id="50fd1-204">It is always non-negative, and **values closer to 0.00 are better**.</span></span> <span data-ttu-id="50fd1-205">可能会无法获得非常小的均方误差值，具体取决于数据。</span><span class="sxs-lookup"><span data-stu-id="50fd1-205">Depending on your data, it may be impossible to get a very small value for the mean squared error.</span></span>|
| <span data-ttu-id="50fd1-206">**RMS 损失**</span><span class="sxs-lookup"><span data-stu-id="50fd1-206">**RMS-loss**</span></span> |  <span data-ttu-id="50fd1-207">[RMS 损失](https://en.wikipedia.org/wiki/Root-mean-square_deviation)或“均方根误差 (RMSE)”  又称为“均方根偏差 (RMSD)”）  ，测量模型预测的值与在建模环境中观测到的值之间的差异。</span><span class="sxs-lookup"><span data-stu-id="50fd1-207">[RMS-loss](https://en.wikipedia.org/wiki/Root-mean-square_deviation) or *Root Mean Squared Error (RMSE)* (also called *Root Mean Square Deviation, RMSD*), measures the difference between values predicted by a model and the values observed from the environment that is being modeled.</span></span> <span data-ttu-id="50fd1-208">RMS 损失是平方损失的平方根，其具有与标签相同的单位，类似于绝对值损失，但赋予了较大的差异更大的权重。</span><span class="sxs-lookup"><span data-stu-id="50fd1-208">RMS-loss is the square root of Squared-loss and has the same units as the label, similar to the absolute-loss though giving more weight to larger differences.</span></span> <span data-ttu-id="50fd1-209">均方根误差通常用于气候学、预测和回归分析，以验证试验结果。</span><span class="sxs-lookup"><span data-stu-id="50fd1-209">Root mean square error is commonly used in climatology, forecasting, and regression analysis to verify experimental results.</span></span> | <span data-ttu-id="50fd1-210">它始终是非负值，并且**越接近 0.00 的值越好**。</span><span class="sxs-lookup"><span data-stu-id="50fd1-210">It is always non-negative, and **values closer to 0.00 are better**.</span></span> <span data-ttu-id="50fd1-211">RMSD 是准确性度量值，用于比较特定数据集的不同模型的预测误差，而不用于比较数据集之间的预测误差，因为它与比例相关。</span><span class="sxs-lookup"><span data-stu-id="50fd1-211">RMSD is a measure of accuracy, to compare forecasting errors of different models for a particular dataset and not between datasets, as it is scale-dependent.</span></span>|

<span data-ttu-id="50fd1-212">有关回归指标的更多详细信息，请阅读以下文章：</span><span class="sxs-lookup"><span data-stu-id="50fd1-212">For further details on regression metrics, read the following articles:</span></span>

- <span data-ttu-id="50fd1-213">[Regression Analysis:How Do I Interpret R-squared and Assess the Goodness-of-Fit?](https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit)（回归分析：我如何解释 R 平方并评估拟合优度？）</span><span class="sxs-lookup"><span data-stu-id="50fd1-213">[Regression Analysis: How Do I Interpret R-squared and Assess the Goodness-of-Fit?](https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit)</span></span>
- <span data-ttu-id="50fd1-214">[How To Interpret R-squared in Regression Analysis](https://statisticsbyjim.com/regression/interpret-r-squared-regression)（如何在回归分析中解释 R 平方）</span><span class="sxs-lookup"><span data-stu-id="50fd1-214">[How To Interpret R-squared in Regression Analysis](https://statisticsbyjim.com/regression/interpret-r-squared-regression)</span></span>
- <span data-ttu-id="50fd1-215">[R-Squared Definition](https://www.investopedia.com/terms/r/r-squared.asp)（R 平方定义）</span><span class="sxs-lookup"><span data-stu-id="50fd1-215">[R-Squared Definition](https://www.investopedia.com/terms/r/r-squared.asp)</span></span>
- <span data-ttu-id="50fd1-216">[Mean Squared Error Definition](https://www.statisticshowto.datasciencecentral.com/mean-squared-error/)（均方误差定义）</span><span class="sxs-lookup"><span data-stu-id="50fd1-216">[Mean Squared Error Definition](https://www.statisticshowto.datasciencecentral.com/mean-squared-error/)</span></span>
- <span data-ttu-id="50fd1-217">[What are Mean Squared Error and Root Mean Squared Error?](https://www.vernier.com/til/1014/)（什么是均方误差和均方根误差？）</span><span class="sxs-lookup"><span data-stu-id="50fd1-217">[What are Mean Squared Error and Root Mean Squared Error?](https://www.vernier.com/til/1014/)</span></span>

## <a name="evaluation-metrics-for-clustering"></a><span data-ttu-id="50fd1-218">聚类分析的评估指标</span><span class="sxs-lookup"><span data-stu-id="50fd1-218">Evaluation metrics for Clustering</span></span>

| <span data-ttu-id="50fd1-219">指标</span><span class="sxs-lookup"><span data-stu-id="50fd1-219">Metric</span></span>   |      <span data-ttu-id="50fd1-220">描述</span><span class="sxs-lookup"><span data-stu-id="50fd1-220">Description</span></span>      |  <span data-ttu-id="50fd1-221">期望</span><span class="sxs-lookup"><span data-stu-id="50fd1-221">Look for</span></span> |
|----------|-----------------------|-----------|
|<span data-ttu-id="50fd1-222">**平均距离**</span><span class="sxs-lookup"><span data-stu-id="50fd1-222">**Average Distance**</span></span>|<span data-ttu-id="50fd1-223">数据点与其分配的群集中心之间的平均距离。</span><span class="sxs-lookup"><span data-stu-id="50fd1-223">Average of the distance between data points and the center of their assigned cluster.</span></span> <span data-ttu-id="50fd1-224">平均距离测量数据点到群集中心的距离。</span><span class="sxs-lookup"><span data-stu-id="50fd1-224">The average distance is a measure of proximity of the data points to cluster centroids.</span></span> <span data-ttu-id="50fd1-225">这是对群集“紧密型”的度量。</span><span class="sxs-lookup"><span data-stu-id="50fd1-225">It's a measure of how 'tight' the cluster is.</span></span>|<span data-ttu-id="50fd1-226">值越接近 0  越好。</span><span class="sxs-lookup"><span data-stu-id="50fd1-226">Values closer to **0** are better.</span></span> <span data-ttu-id="50fd1-227">平均距离越接近零，聚集的数据越多。</span><span class="sxs-lookup"><span data-stu-id="50fd1-227">The closer to zero the average distance is, the more clustered the data is.</span></span> <span data-ttu-id="50fd1-228">但请注意，如果增加了群集的数量，则此指标将减少，在极端情况下（其中每个不同的数据点是其自身的群集），它将等于零。</span><span class="sxs-lookup"><span data-stu-id="50fd1-228">Note though, that this metric will decrease if the number of clusters is increased, and in the extreme case (where each distinct data point is its own cluster) it will be equal to zero.</span></span>
|<span data-ttu-id="50fd1-229">**Davies Bouldin 索引**</span><span class="sxs-lookup"><span data-stu-id="50fd1-229">**Davies Bouldin Index**</span></span>|<span data-ttu-id="50fd1-230">群集内距离与群集间距离之间的平均比率。</span><span class="sxs-lookup"><span data-stu-id="50fd1-230">The average ratio of within-cluster distances to between-cluster distances.</span></span> <span data-ttu-id="50fd1-231">群集越紧密，群集之间的距离越远，此值就越低。</span><span class="sxs-lookup"><span data-stu-id="50fd1-231">The tighter the cluster, and the further apart the clusters are, the lower this value is.</span></span>|<span data-ttu-id="50fd1-232">值越接近 0  越好。</span><span class="sxs-lookup"><span data-stu-id="50fd1-232">Values closer to **0** are better.</span></span> <span data-ttu-id="50fd1-233">距离较远、分散程度较低的群集将获得更好的分数。</span><span class="sxs-lookup"><span data-stu-id="50fd1-233">Clusters that are farther apart and less dispersed will result in a better score.</span></span>|
|<span data-ttu-id="50fd1-234">**规范化相互信息**</span><span class="sxs-lookup"><span data-stu-id="50fd1-234">**Normalized Mutual Information**</span></span>|<span data-ttu-id="50fd1-235">如果用于训练聚类分析模型的训练数据还附带地面真值标签（即监督式聚类分析），则可以使用此指标。</span><span class="sxs-lookup"><span data-stu-id="50fd1-235">Can be used when the training data used to train the clustering model also comes with ground truth labels (that is, supervised clustering).</span></span> <span data-ttu-id="50fd1-236">规范化相互信息指标用于测量是否向相同的群集分配类似的数据点，并将不同的数据点分配给不同群集。</span><span class="sxs-lookup"><span data-stu-id="50fd1-236">The Normalized Mutual Information metric measures whether similar data points get assigned to the same cluster and disparate data points get assigned to different clusters.</span></span> <span data-ttu-id="50fd1-237">规范化相互信息值介于 0 和 1 之间</span><span class="sxs-lookup"><span data-stu-id="50fd1-237">Normalized mutual information is a value between 0 and 1</span></span>|<span data-ttu-id="50fd1-238">值越接近 1  越好</span><span class="sxs-lookup"><span data-stu-id="50fd1-238">Values closer to **1** are better</span></span>|

## <a name="evaluation-metrics-for-ranking"></a><span data-ttu-id="50fd1-239">排名评估指标</span><span class="sxs-lookup"><span data-stu-id="50fd1-239">Evaluation metrics for Ranking</span></span>

| <span data-ttu-id="50fd1-240">指标</span><span class="sxs-lookup"><span data-stu-id="50fd1-240">Metric</span></span>   |      <span data-ttu-id="50fd1-241">描述</span><span class="sxs-lookup"><span data-stu-id="50fd1-241">Description</span></span>      |  <span data-ttu-id="50fd1-242">期望</span><span class="sxs-lookup"><span data-stu-id="50fd1-242">Look for</span></span> |
|----------|-----------------------|-----------|
|<span data-ttu-id="50fd1-243">**折扣累积增益**</span><span class="sxs-lookup"><span data-stu-id="50fd1-243">**Discounted Cumulative Gains**</span></span>|<span data-ttu-id="50fd1-244">折扣累积增益 (DCG) 用于衡量排名质量。</span><span class="sxs-lookup"><span data-stu-id="50fd1-244">Discounted cumulative gain (DCG) is a measure of ranking quality.</span></span> <span data-ttu-id="50fd1-245">它派生自两个假设。</span><span class="sxs-lookup"><span data-stu-id="50fd1-245">It is derived from two assumptions.</span></span> <span data-ttu-id="50fd1-246">1：高相关性项在排名靠前时更有用。</span><span class="sxs-lookup"><span data-stu-id="50fd1-246">One: Highly relevant items are more useful when appearing higher in ranking order.</span></span> <span data-ttu-id="50fd1-247">2：有用性跟踪相关性，即相关性越高，项目越有用。</span><span class="sxs-lookup"><span data-stu-id="50fd1-247">Two: Usefulness tracks relevance that is, the higher the relevance, the more useful an item.</span></span> <span data-ttu-id="50fd1-248">为排名中的特定位置计算折扣累积增益。</span><span class="sxs-lookup"><span data-stu-id="50fd1-248">Discounted cumulative gain is calculated for a particular position in the ranking order.</span></span> <span data-ttu-id="50fd1-249">它使相关性评分除以排名索引的对数，直到达到目标位置。</span><span class="sxs-lookup"><span data-stu-id="50fd1-249">It sums the relevance grading divided by the logarithm of the ranking index up to the position of interest.</span></span> <span data-ttu-id="50fd1-250">它使用 $\sum_{i=0}^{p} \frac {rel_i} {\log_{e}{i+1}}$ 进行计算，关联等级作为地面真值标签提供给排名训练算法。</span><span class="sxs-lookup"><span data-stu-id="50fd1-250">It is calculated using $\sum_{i=0}^{p} \frac {rel_i} {\log_{e}{i+1}}$ Relevance gradings are provided to a ranking training algorithm as ground truth labels.</span></span> <span data-ttu-id="50fd1-251">为排名表中的每个位置提供一个 DCG 值，因此获得名称“折扣累积增益  。</span><span class="sxs-lookup"><span data-stu-id="50fd1-251">One DCG value is provided for each position in the ranking table, hence the name Discounted Cumulative **Gains**.</span></span> |<span data-ttu-id="50fd1-252">**值越大越好**</span><span class="sxs-lookup"><span data-stu-id="50fd1-252">**Higher values are better**</span></span>|
|<span data-ttu-id="50fd1-253">**规范化折扣累积增益**</span><span class="sxs-lookup"><span data-stu-id="50fd1-253">**Normalized Discounted Cumulative Gains**</span></span>|<span data-ttu-id="50fd1-254">对 DCG 进行规范化可以比较不同长度排名列表的指标</span><span class="sxs-lookup"><span data-stu-id="50fd1-254">Normalizing DCG allows the metric to be compared for ranking lists of different lengths</span></span>|<span data-ttu-id="50fd1-255">**值越接近 1 越好**</span><span class="sxs-lookup"><span data-stu-id="50fd1-255">**Values closer to 1 are better**</span></span>|

## <a name="evaluation-metrics-for-anomaly-detection"></a><span data-ttu-id="50fd1-256">异常情况检测评估指标</span><span class="sxs-lookup"><span data-stu-id="50fd1-256">Evaluation metrics for Anomaly Detection</span></span>

| <span data-ttu-id="50fd1-257">指标</span><span class="sxs-lookup"><span data-stu-id="50fd1-257">Metric</span></span>   |      <span data-ttu-id="50fd1-258">描述</span><span class="sxs-lookup"><span data-stu-id="50fd1-258">Description</span></span>      |  <span data-ttu-id="50fd1-259">期望</span><span class="sxs-lookup"><span data-stu-id="50fd1-259">Look for</span></span> |
|----------|-----------------------|-----------|
|<span data-ttu-id="50fd1-260">**ROC 曲线下面积**</span><span class="sxs-lookup"><span data-stu-id="50fd1-260">**Area Under ROC Curve**</span></span>|<span data-ttu-id="50fd1-261">接收方运算符曲线下面积测量模型将异常数据点和常用数据点相分离的程度。</span><span class="sxs-lookup"><span data-stu-id="50fd1-261">Area under the receiver operator curve measures how well the model separates anomalous and usual data points.</span></span>|<span data-ttu-id="50fd1-262">**值越接近 1 越好**。</span><span class="sxs-lookup"><span data-stu-id="50fd1-262">**Values closer to 1 are better**.</span></span> <span data-ttu-id="50fd1-263">只有大于 0.5 的值才能证明模型的有效性。</span><span class="sxs-lookup"><span data-stu-id="50fd1-263">Only values greater than 0.5 demonstrate effectiveness of the model.</span></span> <span data-ttu-id="50fd1-264">0\.5 或更低的值表明，该模型并不优于将输入随机分配到异常和常见类别</span><span class="sxs-lookup"><span data-stu-id="50fd1-264">Values of 0.5 or below indicate that the model is no better than randomly allocating the inputs to anomalous and usual categories</span></span>|
|<span data-ttu-id="50fd1-265">**假正计数的检测率**</span><span class="sxs-lookup"><span data-stu-id="50fd1-265">**Detection Rate At False Positive Count**</span></span>|<span data-ttu-id="50fd1-266">假正计数的检测率是指测试集中正确识别的异常数与异常总数的比率，按每个假正进行索引。</span><span class="sxs-lookup"><span data-stu-id="50fd1-266">Detection rate at false positive count is the ratio of the number of correctly identified anomalies to the total number of anomalies in a test set, indexed by each false positive.</span></span> <span data-ttu-id="50fd1-267">也就是说，每个假正项的假正计数都有一个检测率值。</span><span class="sxs-lookup"><span data-stu-id="50fd1-267">That is, there is a value for detection rate at false positive count for each false positive item.</span></span>|<span data-ttu-id="50fd1-268">**值越接近 1 越好**。</span><span class="sxs-lookup"><span data-stu-id="50fd1-268">**Values closer to 1 are better**.</span></span> <span data-ttu-id="50fd1-269">如果没有假正，则此值为 1</span><span class="sxs-lookup"><span data-stu-id="50fd1-269">If there are no false positives, then this value is 1</span></span>|
